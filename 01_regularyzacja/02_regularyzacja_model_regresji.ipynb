{"cells":[{"cell_type":"markdown","metadata":{"id":"zMo8cxbTCcle"},"source":["# Regularyzacja w modelu regresji - por贸wnanie regresji grzbietowej i regresji Lasso"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgmLVsgCCcln"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from matplotlib.pylab import rcParams\n","rcParams['figure.figsize'] = 12, 10\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import PolynomialFeatures"]},{"cell_type":"markdown","metadata":{"id":"z9-93hQKCclq"},"source":["**Regularyzacja grzbietowa** i **Lasso** s technikami, kt贸re s wykorzystywane do budowania **oszczdnych modeli**, w rozumieniu obecnoci zbyt du偶ej liczby predyktor贸w. \n","Przez du偶 liczb predyktor贸w rozumiemy:\n","\n","- *du偶a liczba predyktor贸w* to taka, kt贸ra prowadzi do **przeuczenia modelu** (ang. *overfitting*) -- nawet tak niewielka liczba jak 10 zmiennych mo偶e prowadzi do przeuczenia,\n","    \n","- *du偶a liczba predyktor贸w* to taka, kt贸ra mo偶e prowadzi do problem贸w z **wydajnoci obliczeniow** -- przy obecnych mo偶liwociach komputer贸w, taka sytuacja mo偶e mie miejsce przy wystpowaniu milion贸w lub miliard贸w cech."]},{"cell_type":"markdown","metadata":{"id":"1t9pE-OECcls"},"source":["Techniki regularyzacyjne dziaaj poprzez \n","- karanie wielkoci wsp贸czynnik贸w cech, \n","- minimalizowanie bdu midzy przewidywanymi a rzeczywistymi obserwacjami."]},{"cell_type":"markdown","metadata":{"id":"JsPjX354Cclt"},"source":["## Dlaczego karamy za wielko wsp贸czynnik贸w?"]},{"cell_type":"markdown","metadata":{"id":"pIq_SG2nCclu"},"source":["Rozwa偶my nastpujcy przykad celem zrozumienia wpywu zo偶onoci modelu na wielko wsp贸czynnik贸w.\n","\n","W tym celu dopasujmy krzyw regresji do krzywej sinusoidalnej (od 0掳 do 360掳) z dodanym szumem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MM2_RDpyCclu"},"outputs":[],"source":["# np.random.seed(123) "]},{"cell_type":"code","source":["# #generacja danych\n","# x = np.array([i*np.pi/180 for i in range(0,360,1)])\n","# X = pd.DataFrame(x)\n","# y = np.sin(x)+np.random.normal(0,0.2,len(x))\n","# plt.plot(x,y,'.',color = 'black')"],"metadata":{"id":"9rqBkfWM7MVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGbJfTobCcly"},"outputs":[],"source":["# #lambda to taka prosta funkcja, bierze dowoln liczb argument贸w, ale mo偶e mie w sobie tylko jedno wyra偶enie\n","# rss_fun  = lambda y, y_pred: sum((y_pred-y)**2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZY2fOOBCcly"},"outputs":[],"source":["# #funkcja tworzy model regresji wielomianowej bez regularyzacji, dopasowuje model do danych i rysuje wykres dla dopasowanych wartoci w modelu\n","# #zwraca RSS, wyraz wolny i reszt wsp贸czynnik贸w \n","# def linear_regression(X, y, power, models_to_plot):\n","#     reg = make_pipeline(PolynomialFeatures(power),\n","#                         StandardScaler(),\n","#                         LinearRegression())\n","#     reg.fit(X, y)\n","#     y_pred = reg.predict(X)\n","    \n","#     if power in models_to_plot:\n","#         plt.subplot(models_to_plot[power])\n","#         plt.tight_layout()\n","#         plt.plot(x, y_pred)\n","#         plt.plot(x, y, '.')\n","#         plt.title('Plot for power: %d' % power)\n","    \n","#     ret = [rss_fun(y, y_pred)]\n","#     ret.extend([reg.named_steps['linearregression'].intercept_])\n","#     ret.extend(reg.named_steps['linearregression'].coef_[1:])\n","#     return ret"]},{"cell_type":"code","source":["# #rysujemy wykres regresji wielomianowej bez regularyzacji, dla r贸偶nych potg\n","# col = ['RSS', 'Intercept'] + ['coef_x_%d' % i for i in range(1, 16)]\n","# ind = ['model_pow_%d' % i for i in range(1, 16)]\n","# coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n","\n","# models_to_plot = {1:231, 3:232, 6:233, 9:234, 12:235, 15:236}\n","\n","# for i in range(1, 16):\n","#     coef_matrix_simple.iloc[i-1, 0:i+2] = linear_regression(X, y, power=i, models_to_plot=models_to_plot)"],"metadata":{"id":"IwoBoC077Xrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.options.display.float_format = '{:,.2g}'.format\n","# coef_matrix_simple"],"metadata":{"id":"P5JZgz3d7s3I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5C1Ugg21Ccl0"},"source":["**Podsumowanie**:\n","\n","- wielko wsp贸czynnik贸w regresji ronie eksponencjalnie wraz ze wzrostem zo偶onoci modelu,\n","- wielko wsp贸czynnika regresji wpywa na istotno zmiennej odpowiadajcej temu wsp贸czynnikowi w oszacowaniu wielko zmiennej odpowiedzi, ale gdy wielko wsp贸czynnik jest zbyt du偶a, algorytm modeluje skomplikowane relacje w celu oszacowania wynik贸w, co czsto koczy si zbytnim dopasowaniem do danych,"]},{"cell_type":"markdown","metadata":{"id":"9zRr3AcfCcl1"},"source":["# Regularyzacja grzbietowa (ang. *rigde regression*)"]},{"cell_type":"markdown","metadata":{"id":"D6kUW89ECcl2"},"source":["Metoda najmniejszych kwadrat贸w z regularyzacj $l2$, minimalizuje **funkcj kryterialn**:\n","\n","$$||y - Xb||^2_2 + \\alpha \\cdot ||b||^2_2,$$\n","\n","gdzie dla dowolnego wektora $n$-wymiarowego $a = (a_1, a_2, \\ldots, a_n)$ zachodzi: $||a||_2 = \\sqrt{\\sum_{i=1}^n a_i^2}$."]},{"cell_type":"markdown","metadata":{"id":"rs2lUs8MCcl2"},"source":["$\\alpha$ - sia regularyzacja, $\\alpha > 0$ \n","\n","* gdy $\\alpha = 0$ -- problem uprasza si do zwykej regresji\n","* gdy $\\alpha = +\\infty$ -- wsp贸czynnik s r贸wne zeru"]},{"cell_type":"markdown","metadata":{"id":"KN43FaERCcl2"},"source":["## Zadanie 1\n","Napisz funkcj, kt贸ra dla dowolnego zbioru ($X$ i $y$) oraz stopnia wielomianu dopasuje model regresji wielomianowej z regularyzacj Ridge z danym parametrem $\\alpha$. Ponadto, funkcja narysuje wykres rozproszenia i dopasowan funkcj regresji dla $k$ danych wartoci parametru $\\alpha$ przy ustalonym stopniu wielomianu (parametr \n","`models_to_plot`).\n","\n","Nastpnie wyznacz ramk danych `coef_matrix_ridge` dla ustalonego stopnia wielomianu (np. 15), kt贸rej wiersze dla ustalonej bd zawieray: warto RSS oraz kolejne wartoci wsp贸czynnik贸w regresji dla r贸偶nych parametr贸w $\\alpha$, np. lista `alpha_ridge`.\n","\n","Sprawd藕 jak zmieniaj si wartoci wsp贸czynnik贸w regresji z regularyzacj grzbietow wraz ze zmian parametru $\\alpha$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsBUISHKCcl3"},"outputs":[],"source":["# #analogiczna funkcja co wczeniej, ale teraz dla Ridge\n","# from sklearn.linear_model import Ridge\n","\n","# def ridge_regression(X, y, alpha, power, models_to_plot={}):\n","#     ridgereg = make_pipeline(PolynomialFeatures(power),\n","#                         StandardScaler(),\n","#                         Ridge(alpha = alpha))\n","#     ridgereg.fit(X, y)\n","#     y_pred = ridgereg.predict(X)\n","    \n","#     if alpha in models_to_plot:\n","#         plt.subplot(models_to_plot[alpha])\n","#         plt.tight_layout()\n","#         plt.plot(x, y_pred)\n","#         plt.plot(x, y, '.')\n","#         plt.title('Plot for alpha: %.3g' % alpha)\n","    \n","#     ret = [rss_fun(y, y_pred)]\n","#     ret.extend([ridgereg.named_steps['ridge'].intercept_])\n","#     ret.extend(ridgereg.named_steps['ridge'].coef_[1:])\n","#     return ret"]},{"cell_type":"code","source":["# #rysujemy wykres regresji wielomianowej z regularyzacj ridge,dla r贸偶nych alph dla potgi 15\n","# alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n","\n","# col = ['RSS','Intercept'] + ['coef_x_%d'%i for i in range(1, 16)]\n","# ind = ['alpha_%.2g' % alpha_ridge[i] for i in range(0, len(alpha_ridge))]\n","# coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n","\n","# models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n","\n","\n","# power = 15\n","# for i in range(10):\n","#     coef_matrix_ridge.iloc[i,] = ridge_regression(X, y, alpha_ridge[i], power, models_to_plot)"],"metadata":{"id":"hIDedOqa9S4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.options.display.float_format = '{:,.2g}'.format\n","# coef_matrix_ridge"],"metadata":{"id":"hsdzdA2N9Vz0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmrznEmACcl4"},"source":["**Podsumowanie**:\n","\n","- wielko RSS (suma kwadrat贸w bd贸w) ronie wraz ze wzrostem wartoci $\\alpha$, wraz z redukcj zo偶onoci modelu,\n","- $\\alpha = 1e-15$ daje istotn redukcj wielkoci wsp贸czynnik贸w regresji,\n","- wy偶sze wartoci $\\alpha$ prowadz do niedouczenia modelu (gwatowny wzrost RSS dla $\\alpha > 1$.\n","- wiele wsp贸czynnik贸w jest bardzo maych, ale nie r贸wnych zeru."]},{"cell_type":"markdown","metadata":{"id":"HHKFGuEiCcl4"},"source":["# Regularyzacja Lasso (ang. *Lasso regression*)"]},{"cell_type":"markdown","metadata":{"id":"vrEjRoNzCcl5"},"source":["LASSO - Least Absolute Shrinkage and Selection Operator"]},{"cell_type":"markdown","metadata":{"id":"NFGw-0xxCcl5"},"source":["Metoda najmniejszych kwadrat贸w z regularyzacj $l1$, minimalizuje **funkcj kryterialn**:\n","\n","$$||y - Xb||^2_2 + \\alpha \\cdot ||b||_1,$$\n","\n","gdzie dla dowolnego wektora $n$-wymiarowego $a = (a_1, a_2, \\ldots, a_n)$ zachodzi: $||a||_1 = \\sum_{i=1}^n |a_i|$."]},{"cell_type":"markdown","metadata":{"id":"0Xr_7uS-Ccl5"},"source":["## Zadanie 2\n","Napisz funkcj, kt贸ra dla dowolnego zbioru ($X$ i $y$) oraz stopnia wielomianu dopasuje model regresji wielomianowej z regularyzacj Lasso z danym parametrem $\\alpha$. Ponadto, funkcja narysuje wykres rozproszenia i dopasowan funkcj regresji dla $k$ danych wartoci parametru $\\alpha$ przy ustalonym stopniu wielomianu (parametr \n","`models_to_plot`).\n","\n","Nastpnie wyznacz ramk danych `coef_matrix_ridge` dla ustalonego stopnia wielomianu (np. 15), kt贸rej wiersze dla ustalonej bd zawieray: warto RSS oraz kolejne wartoci wsp贸czynnik贸w regresji dla r贸偶nych parametr贸w $\\alpha$, np. lista `alpha_ridge`.\n","\n","Sprawd藕 jak zmieniaj si wartoci wsp贸czynnik贸w regresji z regularyzacj Lasso wraz ze zmian parametru $\\alpha$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wUkLNiqCcl5"},"outputs":[],"source":["# #definiujemy funkcj, kt贸ra dopasowuje model lasso dla regresji wielomianowej i tworzy wykresy dla pewnych alph, zwraca wsp贸czynniki i RSS\n","\n","# from sklearn.linear_model import Lasso\n","\n","# def lasso_regression(X, y, alpha, power, models_to_plot={}):\n","#     lassoreg = make_pipeline(PolynomialFeatures(power),\n","#                         StandardScaler(),\n","#                         Lasso(alpha = alpha,max_iter = 1000000))\n","#     lassoreg.fit(X, y)\n","#     y_pred = lassoreg.predict(X)\n","    \n","#     if alpha in models_to_plot:\n","#         plt.subplot(models_to_plot[alpha])\n","#         plt.tight_layout()\n","#         plt.plot(x, y_pred)\n","#         plt.plot(x, y, '.')\n","#         plt.title('Plot for alpha: %.3g' % alpha)\n","    \n","#     ret = [rss_fun(y, y_pred)]\n","#     ret.extend([lassoreg.named_steps['lasso'].intercept_])\n","#     ret.extend(lassoreg.named_steps['lasso'].coef_[1:])\n","#     return ret    "]},{"cell_type":"code","source":["# #rysujemy wykres regresji wielomianowej z regularyzacj lasso, dla r贸偶nych alph dla potgi 15\n","# alpha_lasso = [1e-15, 1e-10, 1e-8,1e-5, 1e-4, 1e-3,1e-2, 1, 5, 10]\n","\n","# col = ['RSS','Intercept'] + ['coef_x_%d'%i for i in range(1, 16)]\n","# ind = ['alpha_%.2g' % alpha_lasso[i] for i in range(0, len(alpha_lasso))]\n","# coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n","\n","# models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n","\n","\n","# power = 15\n","# for i in range(10):\n","#     coef_matrix_lasso.iloc[i,] = lasso_regression(X, y, alpha_lasso[i], power, models_to_plot)"],"metadata":{"id":"LdBzqVj69bHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.options.display.float_format = '{:,.2g}'.format\n","# coef_matrix_lasso"],"metadata":{"id":"V2olveo99euI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fAd4MazCcl6"},"source":["**Podsumowanie**:\n","\n","- wielko RSS (suma kwadrat贸w bd贸w) ronie wraz ze wzrostem wartoci , wraz z redukcj zo偶onoci modelu,\n","- dla tych samym wartoci $\\alpha$, wielko wsp贸czynnik贸w regresji z regularyzacj Lasso jest mniejsza ni偶 wartoci odpowiadajcych wsp贸czynnik贸w w regresji z regularyzacj grzbietow,\n","- dla tych samych wartoci $\\alpha$ regresji z regularyzacj Lasso ma wy偶sze RSS w por贸wnaniu do regresji z regularyzacj grzbietow (gorsze dopasowanie modelu),\n","- wiele wsp贸czynnik贸w jest zerowa (nawet dla niewielkich wielkoci $\\alpha$)."]},{"cell_type":"markdown","metadata":{"id":"PjHLAhpSCcl6"},"source":["## Por贸wnanie regularyzacji grzbietowej z regularyzacj Lasso\n","\n","### Ridge\n","- zawiera wszystkie (lub 偶adne) cechy w modelu, g贸wn zalet tej regularyzacji jest **cigniecie wsp贸czynnik贸w** (ang. **shrinkage coefficient**),\n","- regresji grzbietowej u偶ywa si gowniej do **uniknicia przeuczenia** modelu, ale z racji, 偶e zawiera wszystkie zmienne z modelu nie jest u偶yteczny w przypadku wielowymiarowych danych (gdy liczb predyktor贸w szacuje si milionach/miliardach -- zbyt du偶a zo偶ono obliczeniowa),\n","- zasadniczo dziaa dobrze nawet w obecnoci silnie **skorelowanych** cech -- uwzgldnia wszystkie skorelowane zmienne w modelu, ale wielko wsp贸czynnik贸w zale偶y od wielkoci korelacji.\n","\n","\n","### Lasso \n","- regularyzacja Lasso poza **cigniecie wsp贸czynnik贸w**, dokonuje r贸wnie偶 selekcji zmiennych\n","- regularyzacje Lasso czsto wykorzystuje si do **selekcji zmiennych** w przypadku do liczba cech jest rzdu milion贸w/miliard贸w\n","- wybiera dowoln cech spor贸d cech silnie skorelowanych, wsp贸czynniki pozostaych cechy skorelowanych z wybran zmienn redukuje do zera, ale wybrana zmienna zmienia si losowo wraz ze zmian parametr贸w modelu -- podejcie te dziaa gorzej ni偶 regularyzacja grzbietowa"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}